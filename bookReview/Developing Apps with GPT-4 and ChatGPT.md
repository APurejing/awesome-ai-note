《大模型应用开发极简入门：基于 GPT-4 和 ChatGPT》

> “要学习 AI 技术，我个人认为比较好的方法如下。
> What：系统了解什么是生成式 AI 和大语言模型，大语言模型能做什么，有什么应用场景，局限是什么。
> How：该如何写提示词，如何调用 API。
> Do：把 AI 带入自己的工作和生活，切实去使用 AI。从使用提示词开始，写文案、写总结、翻译、写代码，等等。然后，有能力和想法的朋友，还可以尝试调用 API 去开发 AI 应用程序。”

> 入门指南：“系统地了解什么是大语言模型，大语言模型都有哪些应用场景，以及如何写提示词和调用 API”

> 示例链接：“https://github.com/malywut/gpt_examples”

**大语言模型（large language model，LLM）**
它们能够以非常高的准确性识别和生成人类可读的文本。
“GPT-4 和 ChatGPT 基于一种特定的神经网络架构，即 Transformer。它关注句子或段落的不同部分，以理解其上下文并产生连贯的回答。此外，它还可以理解句子中的单词顺序和上下文意思。这使 Transformer 在语言翻译、问题回答和文本生成等任务中非常有效”

“NLP 是 AI 的一个子领域，专注于使计算机能够处理、解释和生成人类语言。”
“NLP 的目标是让计算机能够处理自然语言文本。这个目标涉及诸多任务”文本分类，自动翻译，问题回答，文本生成

Transformer 则具备高效处理和编码上下文的能力。
这场革命的核心支柱是注意力机制”“交叉注意力和自注意力是基于注意力机制的两个架构模块
Transformer 架构使得 LLM 能够将上下文作为一个整体来考虑。基于这个上下文，模型为每个潜在的后续标记分配一个概率分数，然后选择概率最高的标记作为序列中的下一个标记。
